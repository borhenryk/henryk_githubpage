---
title: ":rocket: Functional Gradient Boosting"
layout: post
date: 2020-08-20 22:48
tag:
- gradient-boosting
- functional-data
- function-on-function-regression
- functional-data-analysis
image: https://borhenryk.github.io/henryk_githubpage/assets/images/Rplot1.png
headerImage: true
projects: true
hidden: true # don't count this post in blog pagination
description: "Gradient Boosting for function on function regression"
category: project
author: henryk
externalLink: false
---


<p align="justify"> In summary, it can be said that regression using Boosting has fundamental advantages over other estimation methods. First, theComponent-wise Boosting scales very well, which means that for an increasing number of observations and variables the computation time does not increase significantly, since you only estimate a small part of the design matrix and therefore never have to invert very large matrices. Second, you can include different variables with different effects in the model.  Thirdly, it is a great advantage to have an automatic variable selection mechanism, which is not the case with conventional methods. Fourthly and lastly,one can model both the expected value and other characteristics of the response, such as the median or any other quantile. This property is due to the fact that different loss functions can be optimized, each producing different properties.</p>

